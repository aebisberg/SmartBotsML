{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Chatbot with Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/35156624/126909072-47c9be9e-549c-420f-ac4b-f9bbd2a4de22.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need to import the dataset for data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_lines = open('movie_lines.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')\n",
    "conversations = open('movie_conversations.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw movie lines:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!',\n",
       " 'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.',\n",
       " 'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?',\n",
       " \"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print()\n",
    "print(\"Raw movie lines:\")\n",
    "print()\n",
    "movie_lines[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw Conversations:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L205', 'L206']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print()\n",
    "print(\"Raw Conversations:\")\n",
    "print()\n",
    "conversations[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dictionary to map movie line and id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_2_movieline = {}\n",
    "for line in movie_lines:\n",
    "    _line = line.split(\" +++$+++ \")\n",
    "    if len(_line) == 5:\n",
    "        id_2_movieline[_line[0]] = _line[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Movie Lines of data set:\n",
      "\n",
      "{'L548581': 'Her...', 'L627784': 'Keep it.', 'L341938': 'Officer Starling. Welcome back.', 'L131568': \"Don't trouble yourself about it, Homer--this ain't your business.\", 'L508193': 'You were unhappy?', 'L265853': 'I..', 'L100213': 'Frannie.', 'L399762': \"What's that?\", 'L553260': \"Oh, some of 'em. But it's mostly just strokes and shit. I mean, most of 'em just sleep all the time and get kind of yellow. Usually they die id they're, you know, really yellow.\"}\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"Movie Lines of data set:\")\n",
    "print()\n",
    "print(dict(list(id_2_movieline.items())[1:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a list of all the conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "List of conversations:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['L194', 'L195', 'L196', 'L197'],\n",
       " ['L198', 'L199'],\n",
       " ['L200', 'L201', 'L202', 'L203'],\n",
       " ['L204', 'L205', 'L206'],\n",
       " ['L207', 'L208'],\n",
       " ['L271', 'L272', 'L273', 'L274', 'L275'],\n",
       " ['L276', 'L277'],\n",
       " ['L280', 'L281'],\n",
       " ['L363', 'L364'],\n",
       " ['L365', 'L366']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations_ids = []\n",
    "for conversation in conversations[:-1]:\n",
    "    _conversation = conversation.split(\" +++$+++ \")[-1][1:-1].replace(\"'\", \"\").replace(\" \", \"\")\n",
    "    conversations_ids.append(_conversation.split(\",\"))\n",
    "print()\n",
    "print(\"List of conversations:\")\n",
    "print()\n",
    "conversations_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split the questions and answers\n",
      "\n",
      "Questions:\n",
      "\n",
      "['Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.', \"Well, I thought we'd start with pronunciation, if that's okay with you.\", 'Not the hacking and gagging and spitting part.  Please.', \"You're asking me out.  That's so cute. What's your name again?\", \"No, no, it's my fault -- we didn't have a proper introduction ---\", 'Cameron.', \"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\", 'Why?', 'Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.', 'Gosh, if only we could find Kat a boyfriend...']\n",
      "\n",
      "Answers:\n",
      "\n",
      "[\"Well, I thought we'd start with pronunciation, if that's okay with you.\", 'Not the hacking and gagging and spitting part.  Please.', \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\", 'Forget it.', 'Cameron.', \"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\", 'Seems like she could get a date easy enough...', 'Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.', \"That's a shame.\", 'Let me see what I can do.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Split the questions and answers\")\n",
    "print()\n",
    "ques = []\n",
    "answ = []\n",
    "for convs in conversations_ids:\n",
    "    for i in range(len(convs) - 1):\n",
    "        ques.append(id_2_movieline[convs[i]])\n",
    "        answ.append(id_2_movieline[convs[i + 1  ]])\n",
    "print(\"Questions:\")\n",
    "print()\n",
    "print(ques[:10])\n",
    "print()\n",
    "print(\"Answers:\")\n",
    "print()\n",
    "print(answ[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we need to clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    \"\"\"\n",
    "    function: clean\n",
    "    params: String text\n",
    "    does: cleans the text removing stop words, punctuation, lower case.\n",
    "    returns: String clean text \n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}+=~|.?,]\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Questions:\n",
      "['can we make this quick  roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad  again', 'well i thought we would start with pronunciation if that is okay with you', 'not the hacking and gagging and spitting part  please', 'you are asking me out  that is so cute what is your name again', \"no no it's my fault  we didn't have a proper introduction \", 'cameron', 'the thing is cameron  i am at the mercy of a particularly hideous breed of loser  my sister  i cannot date until she does', 'why', 'unsolved mystery  she used to be really popular when she started high school then it was just like she got sick of it or something', 'gosh if only we could find kat a boyfriend']\n",
      "\n",
      "Cleaned Answers:\n",
      "['well i thought we would start with pronunciation if that is okay with you', 'not the hacking and gagging and spitting part  please', \"okay then how 'bout we try out some french cuisine  saturday  night\", 'forget it', 'cameron', 'the thing is cameron  i am at the mercy of a particularly hideous breed of loser  my sister  i cannot date until she does', 'seems like she could get a date easy enough', 'unsolved mystery  she used to be really popular when she started high school then it was just like she got sick of it or something', 'that is a shame', 'let me see what i can do']\n"
     ]
    }
   ],
   "source": [
    "clean_ques = []\n",
    "clean_answ = []\n",
    "for question in ques:\n",
    "    clean_ques.append(clean(question))\n",
    "for answer in answ:\n",
    "    clean_answ.append(clean(answer))\n",
    "print()\n",
    "print(\"Cleaned Questions:\")\n",
    "print(clean_ques[:10])\n",
    "print()\n",
    "print(\"Cleaned Answers:\")\n",
    "print(clean_answ[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove less frequent words\n",
    "\n",
    "Find the number of occurunces of each word and remove the lowers 5%, this is to speed up the process of training the data in the neural network and to focus on the most impactful words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word count hash table:\n",
      "\n",
      "{'nineminute': 1, 'palyou': 1, 'delicacy': 4, 'deion': 1, 'demonstrate': 20, 'jimhow': 2, 'angels': 48, 'mafucka': 3, 'rigfort!': 1}\n"
     ]
    }
   ],
   "source": [
    "count_words = {}\n",
    "for ques in clean_ques:\n",
    "    for word in ques.split():\n",
    "        if word not in count_words:\n",
    "            count_words[word] = 1\n",
    "        else:\n",
    "            count_words[word] += 1\n",
    "\n",
    "for answ in clean_answ:\n",
    "    for word in answ.split():\n",
    "        if word not in count_words:\n",
    "            count_words[word] = 1\n",
    "        else:\n",
    "            count_words[word] += 1\n",
    "print()\n",
    "print(\"Word count hash table:\")\n",
    "print()\n",
    "print(dict(list(count_words.items())[1:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and create a threshold \n",
    "Tokenize to get all words and filter out words that do not meet the threshold. The threshold is set at 20%, this hyperparamater can be attuned at different levels to improve the model. Map the words to a unique number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Questions Mapping:\n",
      "\n",
      "{'israel': 2279, 'demonstrate': 0, 'election': 6973, 'angels': 1, 'me': 4406, 'pudding': 5549, 'be!': 1154, 'gekko': 3354, 'beat': 3355}\n",
      "\n",
      "Answers Mapping\n",
      "\n",
      "{'israel': 2279, 'demonstrate': 0, 'election': 6973, 'angels': 1, 'me': 4406, 'pudding': 5549, 'be!': 1154, 'gekko': 3354, 'beat': 3355}\n"
     ]
    }
   ],
   "source": [
    "threshold = 20\n",
    "questions_mapping = {}\n",
    "w_count = 0\n",
    "for word, count in count_words.items():\n",
    "    if count >= threshold:\n",
    "        questions_mapping[word] = w_count\n",
    "        w_count += 1\n",
    "\n",
    "threshold_answ = 20\n",
    "answers_mapping = {}\n",
    "w_count = 0\n",
    "for word, count in count_words.items():\n",
    "    if count >= threshold_answ:\n",
    "        answers_mapping[word] = w_count\n",
    "        w_count += 1\n",
    "\n",
    "print()\n",
    "print(\"Questions Mapping:\")\n",
    "print()\n",
    "print(dict(list(questions_mapping.items())[1:10]))\n",
    "print()\n",
    "print(\"Answers Mapping\")\n",
    "print()\n",
    "print(dict(list(answers_mapping.items())[1:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: LEFT OFF HERE, WORKS ABOVE. \n",
    "tokens = ['<PAD>', '<EOS>', '<OUT>','<SOS>']\n",
    "\n",
    "for token in tokens:\n",
    "    questions_mapping[token] = len(questions_mapping) + 1\n",
    "\n",
    "for token in tokens:\n",
    "    answers_mapping[token] = len(answers_mapping) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_answers = {w_i: w for w, w_i in answers_mapping.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to add the EOS token to end of every answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EOS token at the end of each answer, this is used for the decoding part of the seq2seq model:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['well i thought we would start with pronunciation if that is okay with you <EOS>',\n",
       " 'not the hacking and gagging and spitting part  please <EOS>',\n",
       " \"okay then how 'bout we try out some french cuisine  saturday  night <EOS>\",\n",
       " 'forget it <EOS>',\n",
       " 'cameron <EOS>',\n",
       " 'the thing is cameron  i am at the mercy of a particularly hideous breed of loser  my sister  i cannot date until she does <EOS>',\n",
       " 'seems like she could get a date easy enough <EOS>',\n",
       " 'unsolved mystery  she used to be really popular when she started high school then it was just like she got sick of it or something <EOS>',\n",
       " 'that is a shame <EOS>',\n",
       " 'let me see what i can do <EOS>']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(clean_answ)):\n",
    "    clean_answ[i] += ' <EOS>'\n",
    "print()\n",
    "print(\"EOS token at the end of each answer, this is used for the decoding part of the seq2seq model:\")\n",
    "print()\n",
    "clean_answ[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map the questions and answers for the RNN\n",
    "\n",
    "We need to map the questions and answers to integers in order to train the RNN. This is required as categorical data cannot be trained this way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "quest_mapping_ints = []\n",
    "\n",
    "for ques in clean_ques:\n",
    "    words_map = []\n",
    "    for word in ques.split():\n",
    "        if word not in questions_mapping:\n",
    "            words_map.append(questions_mapping['<OUT>'])\n",
    "        else:\n",
    "            words_map.append(questions_mapping[word])\n",
    "    quest_mapping_ints.append(words_map)\n",
    "    \n",
    "answ_mapping_ints = []\n",
    "\n",
    "for answ in clean_answ:\n",
    "    words_map = []\n",
    "    for word in answ.split():\n",
    "        if word not in answers_mapping:\n",
    "            words_map.append(answers_mapping['<OUT>'])\n",
    "        else:\n",
    "            words_map.append(answers_mapping[word])\n",
    "    answ_mapping_ints.append(words_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Questions Map to integers:\n",
      "\n",
      "[[6796, 8421, 3030, 7358, 3095, 8824, 8824, 2690, 3288, 8824, 3311, 7636, 6595, 6996, 8824, 5685, 6976, 6320, 1276, 1451, 8824, 556], [1681, 3327, 7651, 8421, 7614, 360, 4915, 8824, 6093, 7421, 2977, 4350, 4915, 1873], [3706, 1451, 8824, 2690, 8824, 2690, 8824, 612, 8544], [1873, 3311, 5438, 4406, 5963, 7421, 2977, 6149, 3558, 2224, 2977, 21, 8185, 556], [6143, 6143, 8705, 4940, 6092, 8421, 6594, 5087, 7805, 2774, 8824], [3345], [1451, 523, 2977, 3345, 3327, 7246, 2580, 1451, 3076, 230, 7805, 5181, 8824, 6026, 230, 5658, 4940, 2145, 3327, 1738, 326, 5668, 8406, 5844], [3983], [8824, 920, 8406, 6904, 2782, 8409, 1716, 5102, 2059, 8406, 5355, 2827, 1750, 2839, 5086, 5140, 3376, 2972, 8406, 5297, 3304, 230, 5086, 2502, 3633], [4008, 6093, 2835, 8421, 4894, 7798, 7649, 7805, 4485]]\n",
      "\n",
      "Answers Map to integers:\n",
      "\n",
      "[[1681, 3327, 7651, 8421, 7614, 360, 4915, 8824, 6093, 7421, 2977, 4350, 4915, 1873, 8823], [3706, 1451, 8824, 2690, 8824, 2690, 8824, 612, 8544, 8823], [4350, 2839, 84, 743, 8421, 8784, 5963, 7099, 2671, 8824, 3165, 5338, 8823], [6705, 5086, 8823], [3345, 8823], [1451, 523, 2977, 3345, 3327, 7246, 2580, 1451, 3076, 230, 7805, 5181, 8824, 6026, 230, 5658, 4940, 2145, 3327, 1738, 326, 5668, 8406, 5844, 8823], [4237, 2972, 8406, 4894, 7478, 7805, 326, 8748, 1143, 8823], [8824, 920, 8406, 6904, 2782, 8409, 1716, 5102, 2059, 8406, 5355, 2827, 1750, 2839, 5086, 5140, 3376, 2972, 8406, 5297, 3304, 230, 5086, 2502, 3633, 8823], [7421, 2977, 7805, 2832, 8823], [8436, 4406, 3193, 2224, 3327, 6796, 255, 8823]]\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"Questions Map to integers:\")\n",
    "print()\n",
    "print(quest_mapping_ints[:10])\n",
    "print()\n",
    "print(\"Answers Map to integers:\")\n",
    "print()\n",
    "print(answ_mapping_ints[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to sort the questions and answers by the length of the questions. This will speed up the training in optimization stage. We can set the length of answer and question as 25, as a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sorted Questions: \n",
      "\n",
      "[[3345], [3983], [2631], [6755], [6665], [6143], [4985], [6143], [5218], [1920]]\n",
      "\n",
      "Sorted Answers: \n",
      "\n",
      "[[1451, 523, 2977, 3345, 3327, 7246, 2580, 1451, 3076, 230, 7805, 5181, 8824, 6026, 230, 5658, 4940, 2145, 3327, 1738, 326, 5668, 8406, 5844, 8823], [8824, 920, 8406, 6904, 2782, 8409, 1716, 5102, 2059, 8406, 5355, 2827, 1750, 2839, 5086, 5140, 3376, 2972, 8406, 5297, 3304, 230, 5086, 2502, 3633, 8823], [8289, 8823], [5502, 2972, 119, 3432, 5963, 8222, 1351, 8823], [1873, 8165, 5201, 7358, 6286, 8823], [4350, 1873, 3311, 4443, 4232, 2782, 5253, 84, 2782, 4973, 8823], [166, 3343, 8823], [1873, 3867, 3057, 7396, 5267, 5086, 8823], [6366, 8823], [7614, 1873, 1541, 6336, 4406, 7805, 5626, 3345, 8823]]\n"
     ]
    }
   ],
   "source": [
    "sorted_questions = []\n",
    "sorted_answers = []\n",
    "for length in range(1, 26):\n",
    "    for indx in enumerate(quest_mapping_ints):\n",
    "        if len(indx[1]) == length:\n",
    "            sorted_questions.append(quest_mapping_ints[indx[0]])\n",
    "            sorted_answers.append(answ_mapping_ints[indx[0]])\n",
    "\n",
    "print()\n",
    "print(\"Sorted Questions: \")\n",
    "print()\n",
    "print(sorted_questions[:10])\n",
    "print()\n",
    "print(\"Sorted Answers: \")\n",
    "print()\n",
    "print(sorted_answers[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Seq2Seq Model\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/35156624/129961372-28461497-60d4-4748-81a8-dba523f6a78a.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    \"\"\"\n",
    "    function: model_inputs\n",
    "    params: none\n",
    "    returns: int inputs, int target, float learning rate, float drop_out\n",
    "    \"\"\"\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name = 'input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name = 'target')\n",
    "    lr = tf.placeholder(tf.float32, name = 'learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "    return inputs, targets, lr, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_targets(targets, words_mapping, batch_size):\n",
    "    \"\"\"\n",
    "    function: process_targets\n",
    "    params: targets tenor, hash_words hash table, batch_size int\n",
    "    returns: tensors targets\n",
    "    \"\"\"\n",
    "    left = tf.fill([batch_size, 1], words_mapping['<SOS>'])\n",
    "    right = tf.strided_slice(targets, [0,0], [batch_size, -1], [1,1])\n",
    "    targets = tf.concat([left, right], 1)\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_rnn(inputs, rnn_size, rnn_num_layers, keep_prob, sequence_length):\n",
    "    \"\"\"\n",
    "    function: encoder_layer\n",
    "    params: rnn_inputs, int size number input size, rnn_num_layers int, dropout rate int, seq_len\n",
    "    int length of list in batch\n",
    "    returns: encoder layer\n",
    "    \"\"\"\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    dropout_lstm = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "    encod_cell = tf.contrib.rnn.MultiRNNCell([dropout_lstm] * rnn_num_layers)\n",
    "    encod_output, encoder_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = encod_cell,\n",
    "                                                                    cell_bw = encod_cell,\n",
    "                                                                    sequence_length = sequence_length,\n",
    "                                                                    inputs = inputs,\n",
    "                                                                    dtype = tf.float32)\n",
    "    return encoder_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_train(encode_state, decode_cell, decode_input, sequence_length, decode_scope, output_fun, keep_prob, batch_size):\n",
    "    \"\"\"\n",
    "    function: decoder_training\n",
    "    params: encoder_state returned from encoder_layer, decoder cell in RNN, decoder_input embedding,\n",
    "    seq len int, decode scope, out output function, drop_out drop out rate, batch size int)\n",
    "    returns: decoder output with drop out\n",
    "    \"\"\"\n",
    "    states = tf.zeros([batch_size, 1, decode_cell.output_size])\n",
    "    keys, vals, score_func, attent_func = tf.contrib.seq2seq.prepare_attention(states,\n",
    "                                                                               attention_option = \"bahdanau\",\n",
    "                                                                               num_units = decode_cell.output_size)\n",
    "    train_decoder = tf.contrib.seq2seq.attention_decoder_fn_train(encode_state[0],\n",
    "                                                                              keys,\n",
    "                                                                              vals,\n",
    "                                                                              score_func,\n",
    "                                                                              attent_func,\n",
    "                                                                              name = \"attn_dec_train\")\n",
    "    decode_out, decode_final_state, decode_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decode_cell,\n",
    "                                                                                                train_decoder,\n",
    "                                                                                                decode_input,\n",
    "                                                                                                sequence_length,\n",
    "                                                                                                scope = decode_scope)\n",
    "    decoder_out_dropout = tf.nn.dropout(decode_out, keep_prob)\n",
    "    return output_fun(decoder_out_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_test(encode_state, decode_cell, decode_matrix, sos_id, eos_id, maximum_length, num_words, decode_scope, output_fun, keep_prob, batch_size):\n",
    "    \"\"\"\n",
    "    function: decode_validation_set\n",
    "    params: encoder_state returned from encoder_layer, decoder cell in RNN, decoder_input embedding,\n",
    "    seq len int, decode scope, out output function, drop_out drop out rate, batch size int\n",
    "    returns: test_predictions\n",
    "    \"\"\"\n",
    "    states = tf.zeros([batch_size, 1, decode_cell.output_size])\n",
    "    keys, vals, score_func, attent_func = tf.contrib.seq2seq.prepare_attention(states, attention_option = \"bahdanau\", num_units = decode_cell.output_size)\n",
    "    test_decoder_fun = tf.contrib.seq2seq.attention_decoder_fn_inference(output_fun,\n",
    "                                                                              encode_state[0],\n",
    "                                                                              keys,\n",
    "                                                                              vals,\n",
    "                                                                              score_func,\n",
    "                                                                              attent_func,\n",
    "                                                                              decode_matrix,\n",
    "                                                                              sos_id,\n",
    "                                                                              eos_id,\n",
    "                                                                              maximum_length,\n",
    "                                                                              num_words,\n",
    "                                                                              name = \"attn_dec_inf\")\n",
    "    test_preds, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decode_cell,\n",
    "                                                                                                                test_decoder_fun,\n",
    "                                                                                                                scope = decode_scope)\n",
    "    return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smartbot_rnn(decoder_input, decoder_matrix, encode_state, num_words, sequence_length, rnn_size, num_layers, words_mapping, keep_prob, batch_size):\n",
    "    \"\"\"\n",
    "    function: smartbot_rnn\n",
    "    params: decoder_input, decoder_matrix, encoder_state, total_words_corpus int, seq_len int, rnn_size int,\n",
    "    num_layers_rnn int, hash_words hashtable, drop_out float, batch_size int\n",
    "    returns:\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "        decode_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
    "        weights = tf.truncated_normal_initializer(stddev = 0.1)\n",
    "        biases = tf.zeros_initializer()\n",
    "        output_fun = lambda x: tf.contrib.layers.fully_connected(x,\n",
    "                                                                      num_words,\n",
    "                                                                      None,\n",
    "                                                                      scope = decoding_scope,\n",
    "                                                                      weights_initializer = weights,\n",
    "                                                                      biases_initializer = biases)\n",
    "        training_preds = decode_train(encode_state,\n",
    "                                                   decode_cell,\n",
    "                                                   decoder_input,\n",
    "                                                   sequence_length,\n",
    "                                                   decoding_scope,\n",
    "                                                   output_fun,\n",
    "                                                   keep_prob,\n",
    "                                                   batch_size)\n",
    "        decoding_scope.reuse_variables()\n",
    "        test_preds = decode_test(encode_state,\n",
    "                                           decode_cell,\n",
    "                                           decoder_matrix,\n",
    "                                           words_mapping['<SOS>'],\n",
    "                                           words_mapping['<EOS>'],\n",
    "                                           sequence_length - 1,\n",
    "                                           num_words,\n",
    "                                           decoding_scope,\n",
    "                                           output_fun,\n",
    "                                           keep_prob,\n",
    "                                           batch_size)\n",
    "    return training_preds, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smartbot_model(inputs, targets, keep_prob, batch_size, sequence_length, answers_num_words, questions_num_words, encoder_size, decoder_size, rnn_size, num_layers, questions_mapping):\n",
    "    \"\"\"\n",
    "    function: smartbot_model\n",
    "    params: inputs questions vector, targets answers vector, dropout rate float, batch_size int, seq_len int,\n",
    "    num_words_answers int, num_words_questions int, encoder_size int, decoder_size int, rnn_size int,\n",
    "    rnn_num_layers int, questions_hash hashtable\n",
    "    returns: seq2seq RNN model\n",
    "    \"\"\"\n",
    "    encoder_input = tf.contrib.layers.embed_sequence(inputs,\n",
    "                                                              answers_num_words + 1,\n",
    "                                                              encoder_size,\n",
    "                                                              initializer = tf.random_uniform_initializer(0, 1))\n",
    "    \n",
    "    encoder_state = encoder_rnn(encoder_input, rnn_size, num_layers, keep_prob, sequence_length)\n",
    "    targets = process_targets(targets, questions_mapping, batch_size)\n",
    "    decoder_matrix = tf.Variable(tf.random_uniform([questions_num_words + 1, decoder_size], 0, 1))\n",
    "    decoder_input = tf.nn.embedding_lookup(decoder_matrix, targets)\n",
    "    training_preds, test_preds = smartbot_rnn(decoder_input,\n",
    "                                                         decoder_matrix,\n",
    "                                                         encoder_state,\n",
    "                                                         questions_num_words,\n",
    "                                                         sequence_length,\n",
    "                                                         rnn_size,\n",
    "                                                         num_layers,\n",
    "                                                         questions_mapping,\n",
    "                                                         keep_prob,\n",
    "                                                         batch_size)\n",
    "    return training_preds, test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model - Set up the hyperparamaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 64\n",
    "rnn_size = 512\n",
    "num_layers = 3\n",
    "encoding_embedding_size = 512\n",
    "decoding_embedding_size = 512\n",
    "learning_rate = 0.01\n",
    "learning_rate_decay = 0.9\n",
    "min_learning_rate = 0.0001\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the tensorflow object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "inputs, targets, lr, keep_prob = model_inputs()\n",
    " \n",
    "sequence_length = tf.placeholder_with_default(25, None, name = 'sequence_length')\n",
    " \n",
    "input_shape = tf.shape(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start outputting the training and test predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_preds, test_preds = smartbot_model(tf.reverse(inputs, [-1]),\n",
    "                                                       targets,\n",
    "                                                       keep_prob,\n",
    "                                                       batch_size,\n",
    "                                                       sequence_length,\n",
    "                                                       len(answers_mapping),\n",
    "                                                       len(questions_mapping),\n",
    "                                                       encoding_embedding_size,\n",
    "                                                       decoding_embedding_size,\n",
    "                                                       rnn_size,\n",
    "                                                       num_layers,\n",
    "                                                       questions_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the loss Error and Optimizes. Apply gradient clipping to the optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"optimization\"):\n",
    "    loss = tf.contrib.seq2seq.sequence_loss(training_preds,\n",
    "                                                  targets,\n",
    "                                                  tf.ones([input_shape[0], sequence_length]))\n",
    "    optim = tf.train.AdamOptimizer(learning_rate)\n",
    "    grads = optim.compute_gradients(loss)\n",
    "    clipped_grads = [(tf.clip_by_value(tensor, -5., 5.), var) for tensor, var in grads if tensor is not None]\n",
    "    optimizer_gradient_clipping = optim.apply_gradients(clipped_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(batch_of_sequences, words_hash):\n",
    "    \"\"\"\n",
    "    function: padding\n",
    "    params: batch seqs, hash_words_ints hash table words to integers\n",
    "    returns: sequence with <PAD> token\n",
    "    does: Complete sentences with pad tokens, so all tokens have the same length\n",
    "    \"\"\"\n",
    "    max_sequence_length = max([len(seq) for seq in batch_of_sequences])\n",
    "    return [seq + [words_hash['<PAD>']] * (max_sequence_length - len(seq)) for seq in batch_of_sequences]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_batches(ques, answ, batch_size):\n",
    "    \"\"\"\n",
    "    function: split_into_batches\n",
    "    params: list ques, list ans, int batch size\n",
    "    does: splits data into batches\n",
    "    returns: batches of data\n",
    "    \"\"\"\n",
    "    for batch_index in range(0, len(ques) // batch_size):\n",
    "        start_index = batch_index * batch_size\n",
    "        questions_in_batch = ques[start_index : start_index + batch_size]\n",
    "        answers_in_batch = answ[start_index : start_index + batch_size]\n",
    "        padded_questions_in_batch = np.array(padding(questions_in_batch, questions_mapping))\n",
    "        padded_answers_in_batch = np.array(padding(answers_in_batch, answers_mapping))\n",
    "        yield padded_questions_in_batch, padded_answers_in_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation, for training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the questions and answers into training and validation sets\n",
    "training_validation_split = int(len(sorted_questions) * 0.15)\n",
    "training_ques = sorted_questions[training_validation_split:]\n",
    "training_answ = sorted_answers[training_validation_split:]\n",
    "validation_ques = sorted_questions[:training_validation_split]\n",
    "validation_answ = sorted_answers[:training_validation_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1/3, Batch:    0/2708, Loss Error:  0.089, Time on 100 Batches: 797 seconds\n"
     ]
    }
   ],
   "source": [
    "batch_training_loss = 100\n",
    "batch_valid_loss = ((len(training_ques)) // batch_size // 2) - 1\n",
    "total_training_loss_error = 0\n",
    "list_validation_loss_error = []\n",
    "early_stop = 0\n",
    "early_stopping_stop = 1000\n",
    "checkpoint = \"./chatbot_weights_smartbot.ckpt\" \n",
    "session.run(tf.global_variables_initializer())\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for batch_index, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(training_ques, training_answ, batch_size)):\n",
    "        starting_time = time.time()\n",
    "        _, batch_loss_error = session.run([optimizer_gradient_clipping, loss], {inputs: padded_questions_in_batch,\n",
    "                                                                                               targets: padded_answers_in_batch,\n",
    "                                                                                               lr: learning_rate,\n",
    "                                                                                               sequence_length: padded_answers_in_batch.shape[1],\n",
    "                                                                                               keep_prob: keep_probability})\n",
    "        total_training_loss_error += batch_loss_error\n",
    "        ending_time = time.time()\n",
    "        batch_time = ending_time - starting_time\n",
    "        if batch_index % batch_training_loss == 0:\n",
    "            print('Epoch: {:>3}/{}, Batch: {:>4}/{}, Loss Error: {:>6.3f}, Time on 100 Batches: {:d} seconds'.format(epoch,\n",
    "                                                                                                                                       epochs,\n",
    "                                                                                                                                       batch_index,\n",
    "                                                                                                                                       len(training_ques) // batch_size,\n",
    "                                                                                                                                       total_training_loss_error / batch_training_loss,\n",
    "                                                                                                                                       int(batch_time * batch_training_loss)))\n",
    "            total_training_loss_error = 0\n",
    "        if batch_index % batch_valid_loss == 0 and batch_index > 0:\n",
    "            total_validation_loss_error = 0\n",
    "            starting_time = time.time()\n",
    "            for batch_index_validation, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(validation_ques, validation_answ, batch_size)):\n",
    "                batch_validation_loss_error = session.run(loss_error, {inputs: padded_questions_in_batch,\n",
    "                                                                       targets: padded_answers_in_batch,\n",
    "                                                                       lr: learning_rate,\n",
    "                                                                       sequence_length: padded_answers_in_batch.shape[1],\n",
    "                                                                       keep_prob: 1})\n",
    "                total_validation_loss_error += batch_validation_loss_error\n",
    "            ending_time = time.time()\n",
    "            batch_time = ending_time - starting_time\n",
    "            average_validation_loss_error = total_validation_loss_error / (len(validation_ques) / batch_size)\n",
    "            print('Loss Error: {:>6.3f}, Batch Time: {:d} seconds'.format(average_validation_loss_error, int(batch_time)))\n",
    "            learning_rate *= learning_rate_decay\n",
    "            if learning_rate < min_learning_rate:\n",
    "                learning_rate = min_learning_rate\n",
    "            list_validation_loss_error.append(average_validation_loss_error)\n",
    "            if average_validation_loss_error <= min(list_validation_loss_error):\n",
    "                print('I am improving!')\n",
    "                early_stop = 0\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(session, checkpoint)\n",
    "            else:\n",
    "                print(\"Sorry I need more training.\")\n",
    "                early_stop += 1\n",
    "                if early_stop == early_stopping_stop:\n",
    "                    break\n",
    "    if early_stop == early_stopping_stop:\n",
    "        print(\"Smartbot is ready. Training is complete!\")\n",
    "        break\n",
    "print(\"Smartbot is tired! Goodbye!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_training_loss = 100\n",
    "batch_valid_loss = ((len(training_ques)) // batch_size // 2) - 1\n",
    "total_training_loss_error = 0\n",
    "list_validation_loss_error = []\n",
    "early_stop = 0\n",
    "early_stopping_stop = 1000\n",
    "checkpoint = \"./chatbot_weights.ckpt\" \n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"./chatbot_weights.ckpt\"\n",
    "session = tf.InteractiveSession()\n",
    "session.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(session, checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_words_to_int(question, words_hash):\n",
    "    \"\"\"\n",
    "    fucntion: convert_words_to_int\n",
    "    parms: questions string, words_hash hash table\n",
    "    does: converts words to ints\n",
    "    returns: list ints\n",
    "    \"\"\"\n",
    "    question = clean(question)\n",
    "    return [words_hash.get(word, words_hash['<OUT>']) for word in question.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hello\n",
      "Smartbot:  girl's sod hallout.\n",
      "User: What is the weather?\n",
      "Smartbot:  girl's sod invisible hallout.\n",
      "User: hmmm... needs more training?\n",
      "Smartbot:  girl's sodout.\n",
      "User: Goodbye\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    question = input(\"User: \")\n",
    "    if question == 'Goodbye':\n",
    "        break\n",
    "    question = convert_words_to_int(question, questions_mapping)\n",
    "    question = question + [questions_mapping['<PAD>']] * (25 - len(question))\n",
    "    fake_batch = np.zeros((batch_size, 25))\n",
    "    fake_batch[0] = question\n",
    "    predicted_answer = session.run(test_preds, {inputs: fake_batch, keep_prob: 0.5})[0]\n",
    "    answer = ''\n",
    "    for i in np.argmax(predicted_answer, 1):\n",
    "        if inverse_answers[i] == 'i':\n",
    "            token = ' I'\n",
    "        elif inverse_answers[i] == '<EOS>':\n",
    "            token = '.'\n",
    "        elif inverse_answers[i] == '<OUT>':\n",
    "            token = 'out'\n",
    "        else:\n",
    "            token = ' ' + inverse_answers[i]\n",
    "        answer += token\n",
    "        if token == '.':\n",
    "            break\n",
    "    print('Smartbot: ' + answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of SmartBot!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
